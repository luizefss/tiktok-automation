# /var/www/tiktok-automation/backend/services/gpt_service.py

import os
import json
import logging
from typing import Dict, Any, Optional
import asyncio
import random
import openai
from dotenv import load_dotenv
from config_manager import get_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

config = get_config()


class GPTService:
    def __init__(self):
        self.client = self._initialize_openai()
        # Modelos de topo da OpenAI
        self.available_models = [
            "gpt-4o",
            "gpt-4-turbo"
        ]

    def _initialize_openai(self) -> Optional[openai.AsyncOpenAI]:
        """Inicializa conex√£o com OpenAI API usando a chave do ConfigManager"""
        openai_api_key = config.OPENAI_API_KEY

        if openai_api_key:
            try:
                client = openai.AsyncOpenAI(api_key=openai_api_key)
                logger.info("‚úÖ Conectado ao OpenAI API")
                return client
            except Exception as e:
                logger.error(f"‚ùå Erro ao conectar OpenAI API: {e}")

        logger.warning("‚ö†Ô∏è OpenAI API Key n√£o encontrada")
        return None

    async def generate_script(
        self,
        prompt: str,
        model: Optional[str] = None,
        json_schema: Optional[Dict[str, Any]] = None,
        max_retries: int = 3,
        request_timeout: float = 60.0,
    ) -> Optional[str]:
        """Gera roteiro usando GPT com JSON estrito, retries e fallback de modelo.

        Par√¢metros:
        - prompt: texto do usu√°rio j√° contendo instru√ß√µes de estrutura desejada.
        - model: modelo preferido (opcional). Se falhar, tenta pr√≥ximos da lista.
        - json_schema: schema opcional para for√ßar formato (quando suportado).
        - max_retries: tentativas por modelo antes de trocar de modelo.
        - request_timeout: timeout por requisi√ß√£o em segundos.
        """
        if not self.client:
            logger.error("‚ùå Cliente OpenAI n√£o inicializado")
            return None

        # Ordenar lista de modelos: preferido primeiro, depois os demais
        model_queue = []
        if model and model in self.available_models:
            model_queue.append(model)
        model_queue += [m for m in self.available_models if m not in model_queue]
        if not model_queue:
            logger.error("‚ùå Nenhum modelo dispon√≠vel configurado para OpenAI")
            return None

        last_err: Optional[Exception] = None
        for m in model_queue:
            for attempt in range(max_retries):
                try:
                    # Monta kwargs e for√ßa JSON
                    kwargs: Dict[str, Any] = {
                        "model": m,
                        "messages": [
                            {
                                "role": "system",
                                "content": (
                                    "Responda APENAS com um objeto JSON v√°lido (RFC 8259), sem markdown, sem coment√°rios, "
                                    "sem texto fora do JSON. Voc√™ √© um roteirista especializado em conte√∫do viral para redes sociais."
                                ),
                            },
                            {"role": "user", "content": prompt},
                        ],
                        "max_tokens": 2000,
                        "temperature": 0.7,
                    }

                    # response_format: prioriza schema quando fornecido
                    try:
                        if json_schema:
                            kwargs["response_format"] = {
                                "type": "json_schema",
                                "json_schema": {
                                    "name": "response",
                                    "schema": json_schema,
                                    "strict": True,
                                },
                            }
                        else:
                            kwargs["response_format"] = {"type": "json_object"}
                    except Exception:
                        # Alguns modelos/SDKs podem n√£o suportar, segue sem explicitamente
                        pass

                    # Timeout por tentativa
                    coro = self.client.chat.completions.create(**kwargs)
                    response = await asyncio.wait_for(coro, timeout=request_timeout)

                    if response and getattr(response, "choices", None):
                        content = response.choices[0].message.content
                        logger.info(f"‚úÖ GPT gerou roteiro com sucesso | modelo={m} tentativa={attempt+1}")
                        return content

                    # Se n√£o veio escolha, considera erro controlado para retry
                    raise RuntimeError("Resposta da OpenAI sem choices")

                except Exception as e:
                    last_err = e
                    backoff = min(4.0, (2 ** attempt) * 0.5) + random.random() * 0.2
                    logger.warning(
                        f"‚ö†Ô∏è Falha GPT (modelo={m}, tentativa={attempt+1}/{max_retries}): {e}. Retry em {backoff:.1f}s"
                    )
                    await asyncio.sleep(backoff)

            # Troca de modelo ap√≥s esgotar tentativas
            logger.info(f"üîÅ Trocando de modelo GPT ap√≥s {max_retries} tentativas: {m} ‚Üí pr√≥ximo")

        logger.error(f"‚ùå Falha ao gerar roteiro com GPT ap√≥s {len(model_queue) * max_retries} tentativas: {last_err}")
        return None

    async def check_availability(self, model: str = None) -> bool:
        """Verifica se um modelo espec√≠fico est√° dispon√≠vel"""
        if not self.client:
            return False

        if not model:
            model = self.available_models[0]

        try:
            await self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            return True
        except Exception:
            return False


# Inst√¢ncia global do servi√ßo
gpt_service = GPTService()
